{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "extension2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8yUtnKcdxHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78b9076-a252-4f44-80c0-9a90907b9b1e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from copy import deepcopy\n",
        "import re\n",
        "import string\n",
        "import gensim.models.keyedvectors as word2vec \n",
        "import gensim.downloader as api\n",
        "import gensim.models\n",
        "import tempfile\n",
        "from gensim.models import KeyedVectors\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q6Q34UJf5fJ",
        "outputId": "2dbb2704-c969-45df-f1fd-a04237b4be65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "260Avp97TE3A"
      },
      "source": [
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9hSE0iHkdjh"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    \"\"\" \n",
        "    Expands contractions using regex\n",
        "    Source: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "    \"\"\"\n",
        "    \n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    return phrase"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuB1VqcIj0wY"
      },
      "source": [
        "def clean_text(review):\n",
        "    \"\"\" \n",
        "    Convert text to lower case, expand contractions, remove numbers & punctuation\n",
        "    In milestone 4, we found that this minimal preprocessing performed the best.\n",
        "    \"\"\"\n",
        "    # Cast to lower case\n",
        "    review = review.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    review = decontracted(review)\n",
        "\n",
        "    # Remove numbers\n",
        "    review = re.sub(r'\\d+', ' ', review)\n",
        "\n",
        "    # Remove punctuation \n",
        "    review = re.sub(r'[^\\w\\s]',' ', review)\n",
        "    review = re.sub(r'_',' ', review)\n",
        "\n",
        "    return review\n",
        "\n",
        "def tokenize_text(review, remove_stopword=False):\n",
        "    \"\"\"\n",
        "    Tokenize review after text cleaning using NLTK's word tokenizer\n",
        "    \"\"\"\n",
        "    tokens = nltk.word_tokenize(review)\n",
        "    return tokens"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DhSQdg_Oa8"
      },
      "source": [
        "def calculate_word_occurrences(df):\n",
        "    \"\"\"\n",
        "    Create a dictionary that counts the word occurrences in our data set, where\n",
        "    key = word, value = count of word (among train/test/val dataset)\n",
        "    After counting, we delete infrequent words (words that appear < 2 times)\n",
        "    \"\"\"\n",
        "    data = df.copy(deep=True)\n",
        "    word_counts = Counter()\n",
        "    for idx, row in data.iterrows():\n",
        "        word_counts.update(row['review_tokens'])\n",
        "\n",
        "    print(f\"Number of words before: {len(word_counts.keys())}\")\n",
        "\n",
        "    # Delete infrequent words\n",
        "    for w in set(word_counts):\n",
        "        if word_counts[w] < 2:\n",
        "            del word_counts[w]\n",
        "    \n",
        "    print(f\"Number of words before: {len(word_counts.keys())}\")\n",
        "    \n",
        "    return word_counts"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMGTXZWVB930"
      },
      "source": [
        "def encode_review(review_tokens, word_to_idx, max_len):\n",
        "    \"\"\"\n",
        "    Map each token in the review to their corresponding ID (using the word_to_idx dictionary), up to max_len\n",
        "    If a review has > max_len characters, then we will truncate the review \n",
        "    If a review has < max_len characters, then we will pad it with zeros\n",
        "    \"\"\"\n",
        "    # Initialize the encoded array with 0s\n",
        "    encoded = np.zeros(max_len, dtype=int)\n",
        "\n",
        "    # Get the corresponding ID for each token in the review, using the word_to_idx dictionary\n",
        "    temp = np.array([word_to_idx.get(w, word_to_idx['UNK']) for w in review_tokens])\n",
        "\n",
        "    # Determine whether the review is > max_len\n",
        "    length = min(max_len, len(temp))\n",
        "\n",
        "    # Truncate reviews that are longer than max_len\n",
        "    encoded[:length] = np.copy(temp[:length])\n",
        "\n",
        "    return encoded, length"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giL5H_OyB-1H"
      },
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Create a custom Dataset class for PyTorch\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0]), self.y[idx], self.X[idx][1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-kaPGxKzgZ"
      },
      "source": [
        "def load_vectors(fname):\n",
        "    \"\"\"\n",
        "    Function to load in pretrained word2vec embeddings\n",
        "    For convenience, the pretrained embeddings are downloaded and can be found\n",
        "    in the word2vec folder\n",
        "    \"\"\"\n",
        "    model = KeyedVectors.load(fname, mmap='r')\n",
        "    return model\n",
        "\n",
        "def create_embedding_matrix(word_vecs, word_counts, emb_size=50):\n",
        "    \"\"\" \n",
        "    This function returns: \n",
        "        1. embedding matrix from word2vec embeddings\n",
        "        2. numpy array of vocabulary words\n",
        "        3. word_to_idx dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    # Add 2 for unknown words and padding \n",
        "    vocab_size = len(word_counts) + 2\n",
        "    weights_matrix = np.zeros((vocab_size, emb_size), dtype='float32')\n",
        "\n",
        "    # Vector for padding \n",
        "    weights_matrix[0] = np.zeros(emb_size, dtype='float32')\n",
        "\n",
        "    # Vector for unknown words \n",
        "    weights_matrix[1] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
        "\n",
        "    word_to_idx = {\"\": 0, \"UNK\": 1}\n",
        "    words = [\"\", \"UNK\"]\n",
        "\n",
        "    for idx, word in enumerate(word_counts):\n",
        "        word_to_idx[word] = idx + 2\n",
        "        words.append(word)\n",
        "\n",
        "        if word in word_vecs:\n",
        "            weights_matrix[idx+2] = word_vecs[word]\n",
        "        else:\n",
        "            weights_matrix[idx+2] = np.random.normal(scale=0.6, size=(emb_size, ))\n",
        "    \n",
        "    # Returns weights matrix, array of all words, dictionary that maps from words to index \n",
        "\n",
        "    return weights_matrix, np.array(words), word_to_idx"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN classifier. This can be configured to either RNN or LSTM, and bidirectional or unidirectional.\n",
        "    Reference for creating model: https://github.com/bentrevett/pytorch-sentiment-analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim, num_layers, bidirectional, dropout, rnn_model='lstm'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        \n",
        "        if rnn_model == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional, num_layers=num_layers)\n",
        "        elif rnn_model == 'gru':\n",
        "            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional, num_layers=num_layers)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 5)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x_pack = pack_padded_sequence(x, l, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        if rnn_model == 'lstm':\n",
        "            rnn_out, (ht, ct) = self.rnn(x_pack)\n",
        "        elif rnn_model == 'gru':\n",
        "            rnn_out, ht = self.rnn(x_pack)\n",
        "\n",
        "        if bidirectional:\n",
        "            dense_outputs = self.dropout(torch.cat([ht[-1], ht[-2]], dim=-1))\n",
        "        else:\n",
        "            dense_outputs = self.dropout(ht[-1])\n",
        "\n",
        "        outputs = self.linear(dense_outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "sfpm4zttpRXr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Static variables\n",
        "clip_len = 200\n",
        "emb_size = 300\n",
        "batch_size = 60\n",
        "\n",
        "# IMPORTANT: If you need to rerun this notebook, change your base path to where the data files are located\n",
        "base_path = '/content/drive/MyDrive/530 Project/'"
      ],
      "metadata": {
        "id": "vD1MRFB7Qzbw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in all data\n",
        "train = pd.read_csv(base_path + \"data/train_data.csv\")\n",
        "val = pd.read_csv(base_path + \"data/dev_data.csv\")\n",
        "test = pd.read_csv(base_path + \"data/test_data.csv\")\n",
        "\n",
        "# Concat train, dev, test data for preprocessing\n",
        "data = pd.concat([train, val, test], axis=0, ignore_index=True)\n",
        "data.rename(columns={'Summary': 'review_title'}, inplace=True)\n",
        "\n",
        "# Change ratings to 0-numbering\n",
        "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
        "data['rating'] = data['star_rating'].apply(lambda x: zero_numbering[x])\n",
        "\n",
        "# Text preprocessing\n",
        "data['review_title'] = data['review_title'].fillna('')\n",
        "data['review_body'] = data['review_body'].fillna('')\n",
        "data['review'] = data['review_title'] + ' ' + data['review_body']\n",
        "data['clean_review'] = data['review'].apply(clean_text)\n",
        "\n",
        "# Create tokens after text cleaning\n",
        "data['review_tokens'] = data['clean_review'].apply(tokenize_text)\n",
        "\n",
        "# Calculate review after text cleaning\n",
        "data['review_length'] = data['review_tokens'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "bNfLAwkdQvk1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UJNoSDPb3Sz"
      },
      "source": [
        "# Get word embeddings\n",
        "vector_file = base_path + \"word2vec-google-news-300\"\n",
        "word_vectors = load_vectors(vector_file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwak8VnRKzlg",
        "outputId": "be371e09-a436-4bab-f8d7-a7240da6a4dc"
      },
      "source": [
        "# Count number of occurrences of each word\n",
        "word_counts = calculate_word_occurrences(data)\n",
        "vocab_size = len(word_counts)\n",
        "\n",
        "# Create vocabulary \n",
        "weights_matrix, vocabulary, word_to_idx = create_embedding_matrix(word_vectors, word_counts, emb_size)\n",
        "\n",
        "# Encode text review\n",
        "data['encoded'] = data['review_tokens'].apply(lambda x: encode_review(x, word_to_idx, clip_len))\n",
        "data = data.drop(data[data['review_length'] <= 0].index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words before: 64144\n",
            "Number of words before: 39215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HUSLwfuB-J0"
      },
      "source": [
        "# Prepare PyTorch dataset\n",
        "X = list(data['encoded'])\n",
        "y = list(data['rating'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkEeiTbiJhzr"
      },
      "source": [
        "# Create train/test/dev split using custom Dataset class\n",
        "train_ds = ReviewsDataset(X_train, y_train)\n",
        "val_ds = ReviewsDataset(X_val, y_val)\n",
        "test_ds = ReviewsDataset(X_test, y_test)\n",
        "\n",
        "# Create dataloaders for train/test/dev datasets\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdmAXGt4X16S"
      },
      "source": [
        "def calculate_num_correct(output, y, return_preds=False):\n",
        "    \"\"\" Calculate the count of correct predictions\"\"\"\n",
        "    preds = torch.max(output, 1)[1]\n",
        "    correct = 0\n",
        "    correct = torch.sum(preds == y).item()\n",
        "\n",
        "    if return_preds:\n",
        "        preds_arr = preds.data.cpu().numpy()\n",
        "        np.savetxt('output.csv', preds_arr, fmt='%i')\n",
        "\n",
        "    return correct"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnXWBfcJtx3U"
      },
      "source": [
        "def evaluate_model(model, dataloader, return_preds=False, output_file='y_pred.csv', ytrue_file='y_true.csv'):\n",
        "    \"\"\" Function for evluating the model on the development/test dataset\"\"\"\n",
        "\n",
        "    parameters = model.parameters()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Training step \n",
        "    running_loss = 0\n",
        "    total = 0\n",
        "    num_correct = 0\n",
        "\n",
        "    for i, (encode, label, length) in enumerate(dataloader):\n",
        "\n",
        "        # Send to GPU \n",
        "        encode = encode.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Predictions from the network\n",
        "        output = model(encode, length) \n",
        "        preds = torch.max(output, 1)[1]\n",
        "\n",
        "        # Append true labels and predictions to their corresponding lists\n",
        "        true_labels.extend(label.data.cpu().numpy())\n",
        "        predictions.extend(preds.data.cpu().numpy())\n",
        "\n",
        "        # Compute loss \n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        # Compute number of correctly classified instances \n",
        "        correct = calculate_num_correct(output, label)\n",
        "\n",
        "        running_loss += loss.item() * output.shape[0]\n",
        "        total += output.shape[0]\n",
        "        num_correct += correct\n",
        "        \n",
        "    # Calculate loss and accuracy for the epoch\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = num_correct / total\n",
        "\n",
        "    if return_preds:\n",
        "        np.savetxt(output_file, predictions, fmt='%i')\n",
        "        np.savetxt(ytrue_file, true_labels, fmt='%i')\n",
        "    \n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXEYksc5_qjJ"
      },
      "source": [
        "import time\n",
        "\n",
        "def train_model(model, train_dl, val_dl, epochs=5, lr=0.001):\n",
        "    start = time.time()\n",
        "    print(\"Begin training\")\n",
        "\n",
        "    print_every = 3\n",
        "    parameters = model.parameters()\n",
        "\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_accuracy = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    val_loss_arr = []\n",
        "    val_acc_arr = []\n",
        "\n",
        "    for e in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        # Training step \n",
        "        running_loss = 0\n",
        "        total = 0\n",
        "        num_correct = 0\n",
        "\n",
        "        for i, (encode, label, length) in enumerate(train_dl):\n",
        "\n",
        "            # Send to GPU \n",
        "            encode = encode.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Zero gradients after every batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Predictions from the network\n",
        "            output = model(encode, length)\n",
        "\n",
        "            # Compute loss \n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            # Compute number of correctly classified instances\n",
        "            correct = calculate_num_correct(output, label)\n",
        "\n",
        "            # Backpropage the loss and compute the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * output.shape[0]\n",
        "            total += output.shape[0]\n",
        "            num_correct += correct\n",
        "\n",
        "        # End of epoch\n",
        "        # Calculate loss and accuracy for one epoch\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = num_correct / total\n",
        "\n",
        "        train_loss.append(epoch_loss)\n",
        "        train_acc.append(epoch_acc)\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_dl)\n",
        "\n",
        "        val_loss_arr.append(val_loss)\n",
        "        val_acc_arr.append(val_acc)\n",
        "\n",
        "        # Save model with best dev performance over all epochs\n",
        "        if val_acc > best_accuracy:\n",
        "            best_accuracy = val_acc\n",
        "            best_model = deepcopy(model)\n",
        "            best_epoch = e\n",
        "        \n",
        "        # Print training statistics\n",
        "        if e % print_every == 0:\n",
        "            print(\"----- Train statistics -----\")\n",
        "            print(f\"Epoch {e}\")\n",
        "            print(f\"Train accuracy: {epoch_acc}\")\n",
        "            print(f\"Train loss: {epoch_loss}\")\n",
        "            print()\n",
        "\n",
        "            print(\"----- Validation statistics -----\")\n",
        "            print(f\"Val loss: {val_loss}\")\n",
        "            print(f\"Val accuracy: {val_acc}\")\n",
        "            print()\n",
        "        \n",
        "    # End of training\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"Train accuracy: {train_acc[best_epoch-1]}\")\n",
        "    print(f\"Train loss: {train_loss[best_epoch-1]}\")\n",
        "    print()\n",
        "    print(f\"Dev loss: {val_loss_arr[best_epoch-1]}\")\n",
        "    print(f\"Dev accuracy: {best_accuracy}\")\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total train time (minutes): {(end - start)/60}\")\n",
        "\n",
        "    torch.save(best_model.state_dict(), 'best-model.pt')\n",
        "        \n",
        "    return train_loss, train_acc"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjF56vKpuJNS",
        "outputId": "268fa3b9-4f5b-4915-f9f6-2c1aaacf9b26"
      },
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 300\n",
        "hidden_dim = 300\n",
        "num_layers = 1\n",
        "bidirectional = True\n",
        "dropout = 0.3\n",
        "rnn_model='lstm'\n",
        "\n",
        "model = RNN(vocab_size, embedding_dim, weights_matrix, hidden_dim, num_layers, bidirectional, dropout, rnn_model=rnn_model)\n",
        "model = model.to(device)\n",
        "train_loss, train_acc = train_model(model, train_dl, val_dl, 15, 0.003)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin training\n",
            "----- Train statistics -----\n",
            "Epoch 3\n",
            "Train accuracy: 0.7711517857142857\n",
            "Train loss: 0.582670745248241\n",
            "\n",
            "----- Validation statistics -----\n",
            "Val loss: 0.8855831099408014\n",
            "Val accuracy: 0.6678571428571428\n",
            "\n",
            "----- Train statistics -----\n",
            "Epoch 6\n",
            "Train accuracy: 0.8690446428571429\n",
            "Train loss: 0.3453462529900883\n",
            "\n",
            "----- Validation statistics -----\n",
            "Val loss: 1.1076075327396393\n",
            "Val accuracy: 0.6772857142857143\n",
            "\n",
            "----- Train statistics -----\n",
            "Epoch 9\n",
            "Train accuracy: 0.8933482142857143\n",
            "Train loss: 0.28319150510510166\n",
            "\n",
            "----- Validation statistics -----\n",
            "Val loss: 1.2962410075323922\n",
            "Val accuracy: 0.6725714285714286\n",
            "\n",
            "----- Train statistics -----\n",
            "Epoch 12\n",
            "Train accuracy: 0.8981517857142857\n",
            "Train loss: 0.2718177162050935\n",
            "\n",
            "----- Validation statistics -----\n",
            "Val loss: 1.3895961623532431\n",
            "Val accuracy: 0.6695\n",
            "\n",
            "----- Train statistics -----\n",
            "Epoch 15\n",
            "Train accuracy: 0.8956875\n",
            "Train loss: 0.2777675553963387\n",
            "\n",
            "----- Validation statistics -----\n",
            "Val loss: 1.4125259410483497\n",
            "Val accuracy: 0.6663571428571429\n",
            "\n",
            "Best epoch: 5\n",
            "Train accuracy: 0.8496964285714286\n",
            "Train loss: 0.390747402257153\n",
            "\n",
            "Dev loss: 1.0558373939990997\n",
            "Dev accuracy: 0.6792142857142857\n",
            "Total train time (minutes): 17.965802093346912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Up3PxqG44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601ea605-2970-4782-9f9e-44f631932099"
      },
      "source": [
        "# Load in model with best dev accuracy\n",
        "best_model = RNN(vocab_size, embedding_dim, weights_matrix, hidden_dim, num_layers, bidirectional, dropout, rnn_model=rnn_model)\n",
        "best_model = best_model.to(device)\n",
        "best_model.load_state_dict(torch.load('/content/best-model.pt'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeOlcIgt9rBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2731b3c-6669-4b2a-e47d-2ce021b665a6"
      },
      "source": [
        "# PRINT VALIDATION STATISTICS\n",
        "\n",
        "val_loss, val_acc = evaluate_model(best_model, val_dl, return_preds=True, output_file='val_ypred.csv', ytrue_file='val_ytrue.csv')\n",
        "\n",
        "val_output_csv = pd.read_csv('val_ypred.csv', header=None)\n",
        "val_true_y = pd.read_csv('val_ytrue.csv', header=None)\n",
        "\n",
        "print(len(val_output_csv) == len(val_true_y))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Validation accuracy: ', accuracy_score(val_true_y, val_output_csv))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Validation Average F1:', f1_score(val_true_y, val_output_csv, average=\"weighted\"))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Validation RMSE: ', np.sqrt(mean_squared_error(val_true_y, val_output_csv)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\n",
            "Validation accuracy:  0.6792142857142857\n",
            "\n",
            "Validation Average F1: 0.6798049488054211\n",
            "\n",
            "Validation RMSE:  0.7743661002025186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXa8f4oohYX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb111cf-496a-4ce0-aa78-5626f1f81ed5"
      },
      "source": [
        "# PRINT TEST STATISTICS\n",
        "test_loss, test_acc = evaluate_model(best_model, test_dl, return_preds=True)\n",
        "\n",
        "output_csv = pd.read_csv('y_pred.csv', header=None)\n",
        "true_y = pd.read_csv('y_true.csv', header=None)\n",
        "\n",
        "print(len(output_csv) == len(true_y))\n",
        "\n",
        "print('Test accuracy: ', accuracy_score(true_y, output_csv))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Test Average F1:', f1_score(true_y, output_csv, average=\"weighted\"))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Test RMSE: ', np.sqrt(mean_squared_error(true_y, output_csv)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Test accuracy:  0.6761428571428572\n",
            "\n",
            "Test Average F1: 0.6769670777196883\n",
            "\n",
            "Test RMSE:  0.7832624081366346\n"
          ]
        }
      ]
    }
  ]
}